{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68621642-9d1f-4576-a376-0bbc2734d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"06b-anime-degan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb6da9-cc3a-4de3-9e7a-3dac0fd69c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opendatasets as od\n",
    "\n",
    "# dataset_url = \"https://www.kaggle.com/splcher/animefacedataset\"\n",
    "# od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610c1dc-eee0-4671-8c17-3cb412953d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"./animefacedataset\"\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cc3c1-e409-49be-8bb3-1625e5d88574",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(DATA_DIR + \"/images\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85b2106-0b07-4393-8c66-b8fcad6dc4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fa0c5-8bb4-4df6-940b-9bff3ca13194",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "batch_size = 128\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ca081-cfcb-4bc6-a3ab-1a7472af69fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
    "    T.Resize(image_size),\n",
    "    T.CenterCrop(image_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)]))  # data augementation.\n",
    "# print(train_ds[0][0].shape)  # torch.Size([3, 64, 64])\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b06f475-0e20-4e1e-a018-4444a21d6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af9e28-1fd6-4290-aef9-610b259a61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(img_tensors):  # normalization: (x - avg) / variance\n",
    "    return img_tensors * stats[1][0] + stats[0][0]  # data/image denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c20b32a-187c-4584-a823-41ad54c9a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))  # pyplot.subplot > figure.subplots() > gridspec.subplots() > np.empty(), with element as figure.add_subplot() > returns ~.axes.Axes to the figure as part of a subplot arrangement.\n",
    "    print(\">>>: \", type(ax), ax)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    # print(\">>>>: \", images[:nmax])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))  # display data as an image, i.e., on a 2D regular raster.\n",
    "    # make_grid, returns the tensor containing grid of images.\n",
    "    # imshow, displays data as an image, i.e., on a 2D regular raster.\n",
    "\n",
    "def show_batch(dl, nmax=64):\n",
    "    for images, _ in dl:  # the dl is composed of batches, _ is the label tensor.\n",
    "        # print(\"<><><>: \", type(_), _)\n",
    "        show_images(images, nmax)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97528ea-4909-4351-a6de-6070775afea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e31a0-f056-476d-976e-da78e65cf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bf3f9-7ef1-4306-94db-a1231a521bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee45372-397d-4c6e-9dbc-0cf7168d712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581e4d8-9aa8-43b3-8a41-e789efe88875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_dl), \"\\n\", train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31431027-fd33-4b70-80a1-bc6969039476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b727645-fd43-4849-91a7-9d8328a1990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the formula to calculate the size of the output of a module.\n",
    "discriminator = nn.Sequential(  # the original image's size is 3 x 64 x 64\n",
    "    # in: 3 x 64 x 64\n",
    "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    # Conv2d.__init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\n",
    "    nn.BatchNorm2d(64),  # 64 is num_features, the number of features.\n",
    "    nn.LeakyReLU(0.2, inplace=True),  # modified ReLU, which gived a slope of 0.2 to the variable when it is less than 0.\n",
    "    # out: 64 x 32 x 32\n",
    "    \n",
    "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 128 * 16 * 16\n",
    "    \n",
    "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 256 x 8 x 8\n",
    "    \n",
    "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.LeakyReLU(0.2, inplace=True),\n",
    "    # out: 512 x 4 x 4\n",
    "    \n",
    "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    # out: 1 x 1 x 1\n",
    "    nn.Flatten(),  # flattens a contiguous range of dims into a tensor.\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a979ac4-982b-4af8-92bd-121762a6f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = to_device(discriminator, device)  # put the models into device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc95517-13f0-4d1d-974a-517d8b67c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c4c09-c27e-4013-bc9f-54e7862da731",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.Sequential(\n",
    "    # in: latent_size x 1 x 1\n",
    "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "\n",
    "    nn.BatchNorm2d(512),\n",
    "    nn.ReLU(True),\n",
    "    # out: 512 x 4 x 4\n",
    "\n",
    "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(True),\n",
    "    # out: 256 x 8 x 8\n",
    "\n",
    "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(True),\n",
    "    # out: 128 x 16 x 16\n",
    "\n",
    "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(True),\n",
    "    # out: 64 x 32 x 32\n",
    "\n",
    "    # ConvTranspose2d is like reverse operation of Conv2d module.\n",
    "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),  # ConvTranspose2d.__init__(self, in_channels, out_channels, kernel_size, ...)\n",
    "    nn.Tanh()\n",
    "    # out : 3 x 64 x 64\n",
    ")\n",
    "\"\"\"\n",
    "ConvTranspose2d, \n",
    "    Applies a 2D transposed convolution operator over an input image composed of several input planes.\n",
    "    This is set so that\n",
    "    when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`\n",
    "    are initialized with same parameters, they are inverses of each other in\n",
    "    regard to the input and output shapes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77b52a1-6579-45cb-97c3-59d8a8867054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find out how the multi-dimensional matrix initialization\n",
    "xb = torch.randn(batch_size, latent_size, 1, 1)  # random latent tensors\n",
    "# print(\">> xb is: \", xb)\n",
    "print(\"xb size is :\", xb.shape)\n",
    "fake_images = generator(xb)\n",
    "print(fake_images.shape)  # batch_size, channels, rows, cols; torch.Size([128, 3, 64, 64])\n",
    "show_images(fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb238ee-070c-4389-8fdd-25440495f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = to_device(generator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbec1e9-db50-4c6d-922e-20c7c0df95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(real_images, opt_d):  # Note: discriminator is the module, while train_discriminator is the process to train the module.\n",
    "    # here, real_images is a batch of images.\n",
    "    # Clear discriminator gradients\n",
    "    opt_d.zero_grad()\n",
    "\n",
    "    # Pass real images through discriminator\n",
    "    real_preds = discriminator(real_images)\n",
    "    real_targets = torch.ones(real_images.size(0), 1, device=device)  # TODO take a look at this ones method.\n",
    "    # tensor.size(), returns the size of the tensor.\n",
    "    real_loss = F.binary_cross_entropy(real_preds, real_targets)  # based on the LI's tutorial, the result is not gonna be good.\n",
    "    real_score = torch.mean(real_preds).item()\n",
    "\n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "\n",
    "    # Pass fake images through discriminator\n",
    "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n",
    "    fake_preds = discriminator(fake_images)  # the return of discriminator is a number between 0 and 1.\n",
    "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
    "    fake_score = torch.mean(fake_preds).item()\n",
    "\n",
    "    # Update discriminator weights\n",
    "    loss = real_loss + fake_loss\n",
    "    loss.backward()  # this function is used to compute the gradient of current tensor wrt graph leaves.\n",
    "    opt_d.step()  # execute the optimization once. # after this step, the new weight and bias parameters will be saved in this module chain.\n",
    "    return loss.item(), real_score, fake_score  # loss.item(), returns the value of the tensor as the standard Python number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cd6a43-5024-447a-8058-2175c9f3c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(opt_g):\n",
    "    # Clear generator gradients\n",
    "    opt_g.zero_grad()\n",
    "\n",
    "    # Generate fake images\n",
    "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n",
    "    fake_images = generator(latent)\n",
    "\n",
    "    # Try to fool the discriminator \n",
    "    preds = discriminator(fake_images)\n",
    "    targets = torch.ones(batch_size, 1, device=device)  # why here the targets is set to ones, other than zeros. A: here, the loss can denote the difference between the real image and fake image, since the discriminator will tag the real image with 1.\n",
    "    # therefore, the loss can be the difference between the prediction result and 1.\n",
    "    loss = F.binary_cross_entropy(preds, targets)\n",
    "\n",
    "    # Update generator weights\n",
    "    loss.backward()\n",
    "    opt_g.step()\n",
    "\n",
    "    return loss.item()  # here convert the tensor to the standard python number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e483df5-ea16-4083-b773-73a6eff1e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0cc0f-901e-4b87-bd5b-9dc206b8d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = \"generated\"\n",
    "os.makedirs(sample_dir, exist_ok=True)  # Note: the method is `makedirs`, not makedir or mkdirs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b609377-7000-4fc2-a83c-df82aee88a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(index, latent_tensors, show=True):\n",
    "    fake_images = generator(latent_tensors)  # latent_tensors is like seed in random number generator.\n",
    "    fake_fname = \"generated-images-{0:0=4d}.png\".format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)  # nrow, number of images displayed in each row of the grid.\n",
    "    print(\"Saving\", fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))  # tensor.cpu(), returns a copy of this object in CPU memory.\n",
    "        # tensor.detach(), returns a new tensor, detached from the current graph. the result will never require gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b6e93-16e0-4300-b6ca-98bd6f3fb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)  # latent_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ca1dbb-ab37-4e0d-a44e-0ba8ae116d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_samples(0, fixed_latent)  # in this function, it will use generator to generate the fake image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2487e85-5b06-4e3b-82d4-7db484746261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm  # experimental IPython/Jupyter Notebook widget using tqdm!\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e8df1-2d58-4b9d-8603-0600897c6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, start_idx=1):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    losses_g = []\n",
    "    losses_d = []\n",
    "    real_scores = []\n",
    "    fake_scores = []\n",
    "\n",
    "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))  # module.parameters(), returns an iterator.\n",
    "    # betas, coefficients used for computing running averages of gradient and its square.\n",
    "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))  # this is an optimizer, which will be put in the training model.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_images, _ in tqdm(train_dl):\n",
    "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
    "            loss_g = train_generator(opt_g)\n",
    "        losses_g.append(loss_g)\n",
    "        losses_d.append(loss_d)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        print(\"Epoch [{} / {}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
    "    return losses_g, losses_d, real_scores, fake_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a109dbc6-0313-4cb8-9cba-cef937583431",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa45a9-c7c8-4207-95bc-a1b519964a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f0259-06a3-43fa-96ff-21cb30bf814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_g, losses_d, real_scores, fake_scores = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f0720-e69d-4adb-bf06-ea095fe260a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4bd19b-96de-4ead-91aa-d3858a3ba5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bcf7f-360e-43ca-bea9-84e7cbd2053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0005.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520fba5-d84f-4a95-8267-ca2dd8895dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0010.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60fda8-8dd6-42b7-a5ba-6a88b8ca30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0020.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df980f-f8a2-4547-9d60-b7aadda8eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./generated/generated-images-0025.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778db68-93da-4ec5-b6de-2dfc8829e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "vid_fname = 'gans_training.avi'\n",
    "\n",
    "files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\n",
    "files.sort()\n",
    "\n",
    "out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n",
    "[out.write(cv2.imread(fname)) for fname in files]\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27265437-cfa0-4943-b0d8-252da9e95feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_d, '-')\n",
    "plt.plot(losses_g, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4abd1-b950-443a-870a-f4227bd5d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_scores, '-')\n",
    "plt.plot(fake_scores, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "plt.legend(['Real', 'Fake'])\n",
    "plt.title('Scores');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ccfaf-e61b-40a9-97f0-5cb7536bd37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
